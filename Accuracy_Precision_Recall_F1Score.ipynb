{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Accuracy: (%) of total correct predictions.\n",
    "Accuracy= correct_Prediction/Total_Prediction *100 (%)\n",
    "It reflects how accurately the model predicts the positive class.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Precision= It measures how many of the items predicted positives are acutually positive.\n",
    "Precision= TP/(TP+FP) *100 (%)\n",
    "TP=True Positive FP=False Positive\n",
    "example: In email spam detection, precision measures how many of \n",
    "the emails classified as spam are actually spam.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Recall: It measures how many of the actual positives are correctly identified by the model.\n",
    "Recall= TP/(TP+FN) *100 (%)\n",
    "FN=False Negative\n",
    "Example:If there are 12 spam emails in total and the filter correctly identifies 8 of them as spam\n",
    "recall= 8/(8+4)= 0.66= 66%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" F1 score= It is a metric used in ML to evaluate the performance of a classification model by\n",
    "combining both precision and recall in a single number. Useful when data is imbalanced or biased. \n",
    "Range= 0-1\n",
    "F1 score= 2*(Precision*Recall)/(Precision+Recall)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
